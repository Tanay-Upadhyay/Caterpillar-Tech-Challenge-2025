{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magnetbrains-bit/Caterpillar-Tech-Challenge-2025/blob/main/CATERPILLAR_AI_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "852GQtZHDfNz",
        "outputId": "ccffb40f-e902-4abc-8a08-3ab5b869171e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok Authtoken variable set (ensure it's your valid token).\n",
            "\n",
            "Installing libraries (including ultralytics for YOLO)...\n",
            "Library installation complete.\n",
            "\n",
            "Repository 'Python-Depth-Est-AV2' already exists. Skipping clone.\n",
            "\n",
            "Already in directory: /content/Python-Depth-Est-AV2\n",
            "\n",
            "Configuring ngrok with the provided token...\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Ngrok configuration command executed.\n",
            "\n",
            "✅ Cell 1: Setup, Cloning, and Libs Installation Complete.\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title 1. Setup Environment, Clone Repository & Install Streaming Libs\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torchvision')\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"2wCr4ur5cPXoyb0P92RPsXHGdMM_7q6AMsVz2sYZHyp67YpBK\" # <<< REPLACE THIS\n",
        "print(\"Ngrok Authtoken variable set (ensure it's your valid token).\")\n",
        "\n",
        "print(\"\\nInstalling libraries (including ultralytics for YOLO)...\")\n",
        "!pip install -q opencv-python-headless torch torchvision torchaudio ultralytics websockets pyngrok nest_asyncio matplotlib numpy xformers\n",
        "print(\"Library installation complete.\")\n",
        "\n",
        "GIT_REPO_URL = \"https://github.com/computervisionpro/Python-Depth-Est-AV2.git\"\n",
        "REPO_NAME = GIT_REPO_URL.split('/')[-1].replace('.git', '')\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"\\nCloning repository: {GIT_REPO_URL}\")\n",
        "    !git clone {GIT_REPO_URL}\n",
        "else:\n",
        "    print(f\"\\nRepository '{REPO_NAME}' already exists. Skipping clone.\")\n",
        "\n",
        "target_dir = f\"/content/{REPO_NAME}\"\n",
        "if os.path.isdir(target_dir):\n",
        "    if os.getcwd() != target_dir:\n",
        "        try:\n",
        "            %cd {target_dir}\n",
        "            print(f\"\\nChanged directory to: {os.getcwd()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error changing directory: {e}. Current dir: {os.getcwd()}\")\n",
        "    else:\n",
        "        print(f\"\\nAlready in directory: {os.getcwd()}\")\n",
        "    if os.getcwd() not in sys.path:\n",
        "        sys.path.append(os.getcwd())\n",
        "        print(f\"Added '{os.getcwd()}' to sys.path\")\n",
        "else:\n",
        "    print(f\"!!! ERROR: Target directory '{target_dir}' does not exist. Cloning might have failed.\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != \"YOUR_NGROK_AUTH_TOKEN_HERE\" and NGROK_AUTH_TOKEN.strip() != \"\":\n",
        "    print(f\"\\nConfiguring ngrok with the provided token...\")\n",
        "    try:\n",
        "        !ngrok config add-authtoken {NGROK_AUTH_TOKEN}\n",
        "        print(\"Ngrok configuration command executed.\")\n",
        "    except Exception as ngrok_err:\n",
        "        print(f\"ERROR during ngrok configuration: {ngrok_err}\")\n",
        "else:\n",
        "    print(\"\\nSkipping ngrok configuration due to missing or placeholder token.\")\n",
        "\n",
        "print(\"\\n✅ Cell 1: Setup, Cloning, and Libs Installation Complete.\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cARpclddHF0C",
        "outputId": "8be39f45-dd51-4d50-9e69-7ef76f3f0b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth Model Type: vits\n",
            "Depth Checkpoint 'checkpoints/depth_anything_v2_vits.pth' already exists.\n",
            "Using device: cuda for PyTorch model loading.\n",
            "Imported DepthAnythingV2.\n",
            "  DepthAnythingV2 has no img_size attribute after init!\n",
            "Depth model weights loaded.\n",
            "Depth model moved to cuda and set to eval mode.\n",
            "\n",
            "Loading YOLO model: yolov8s.pt...\n",
            "YOLO model 'yolov8s.pt' loaded.\n",
            "----------------------------------------------------------------------\n",
            "✅ Depth Model 'vits' Ready.\n",
            "✅ YOLO Model 'yolov8s.pt' Ready.\n",
            "Global vars _depth_model, _yolo_model, MODEL_TYPE, DEVICE, model_configs should be set.\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Download Depth Model Checkpoint and Load Models (Depth + YOLO)\n",
        "import torch\n",
        "import torch.nn as nn # Though nn might not be directly used here, often good to have with torch\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Ensure global model placeholders for other cells\n",
        "_depth_model = None\n",
        "_yolo_model = None\n",
        "MODEL_TYPE = \"vits\" # As per your OCR examples\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_configs = {} # Will be populated\n",
        "\n",
        "# Ensure Repo Path if Cell 1's %cd didn't persist for python kernel\n",
        "if 'REPO_NAME' not in globals(): REPO_NAME = \"Python-Depth-Est-AV2\" # Fallback\n",
        "repo_path_check = f\"/content/{REPO_NAME}\"\n",
        "if os.path.exists(repo_path_check) and repo_path_check not in sys.path:\n",
        "    sys.path.append(repo_path_check)\n",
        "    print(f\"[Cell 2] Added {repo_path_check} to sys.path\")\n",
        "if os.path.isdir(repo_path_check) and os.getcwd() != repo_path_check:\n",
        "    try:\n",
        "        os.chdir(repo_path_check)\n",
        "        print(f\"[Cell 2] Changed directory to {os.getcwd()}\")\n",
        "    except Exception as e_chdir:\n",
        "        print(f\"[Cell 2 Warning] Could not change dir: {e_chdir}\")\n",
        "\n",
        "# === Depth Model Configuration and Loading (from your OCR structure) ===\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "CHECKPOINT_NAME = f\"depth_anything_v2_{MODEL_TYPE}.pth\"\n",
        "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, CHECKPOINT_NAME)\n",
        "print(f\"Depth Model Type: {MODEL_TYPE}\")\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "if MODEL_TYPE == 'vits': hf_model_name = 'Small'\n",
        "elif MODEL_TYPE == 'vitb': hf_model_name = 'Base'\n",
        "elif MODEL_TYPE == 'vitl': hf_model_name = 'Large'\n",
        "elif MODEL_TYPE == 'vitg': hf_model_name = 'Giant'\n",
        "else: hf_model_name = MODEL_TYPE.upper()\n",
        "DOWNLOAD_URL = f\"https://huggingface.co/depth-anything/Depth-Anything-V2-{hf_model_name}/resolve/main/{CHECKPOINT_NAME}\"\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_PATH) or os.path.getsize(CHECKPOINT_PATH) < 1000:\n",
        "    print(f\"Downloading depth checkpoint: {DOWNLOAD_URL}...\")\n",
        "    download_status = os.system(f'wget -O \"{CHECKPOINT_PATH}\" \"{DOWNLOAD_URL}\"')\n",
        "    if download_status != 0 or not (os.path.exists(CHECKPOINT_PATH) and os.path.getsize(CHECKPOINT_PATH) > 1000):\n",
        "        print(f\"!!! ERROR: Failed download checkpoint. wget status: {download_status}\")\n",
        "    else:\n",
        "        print(\"Download complete.\")\n",
        "else:\n",
        "    print(f\"Depth Checkpoint '{CHECKPOINT_PATH}' already exists.\")\n",
        "\n",
        "model_configs = { # Made global for other cells if they need it\n",
        "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
        "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
        "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
        "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
        "}\n",
        "print(f\"Using device: {DEVICE} for PyTorch model loading.\")\n",
        "\n",
        "depth_model_loader = None # Local var for loading\n",
        "if os.path.exists(CHECKPOINT_PATH) and os.path.getsize(CHECKPOINT_PATH) > 0:\n",
        "    try:\n",
        "        from depth_anything_v2.dpt import DepthAnythingV2\n",
        "        print(\"Imported DepthAnythingV2.\")\n",
        "        if MODEL_TYPE not in model_configs:\n",
        "            raise ValueError(f\"Unsupported MODEL_TYPE '{MODEL_TYPE}'\")\n",
        "\n",
        "        config = model_configs[MODEL_TYPE]\n",
        "        depth_model_loader = DepthAnythingV2(\n",
        "            encoder=config['encoder'],\n",
        "            features=config['features'],\n",
        "            out_channels=config['out_channels']\n",
        "        )\n",
        "        # Debug after init\n",
        "        if hasattr(depth_model_loader, 'img_size'):\n",
        "            print(f\"  DepthAnythingV2.img_size after init: {depth_model_loader.img_size}\")\n",
        "        else: print(\"  DepthAnythingV2 has no img_size attribute after init!\")\n",
        "\n",
        "        state_dict = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
        "        depth_model_loader.load_state_dict(state_dict, strict=False)\n",
        "        print(\"Depth model weights loaded.\")\n",
        "        depth_model_loader = depth_model_loader.to(DEVICE).eval()\n",
        "        print(f\"Depth model moved to {DEVICE} and set to eval mode.\")\n",
        "    except Exception as e_depth:\n",
        "        print(f\"!!! ERROR loading depth model: {e_depth}\")\n",
        "        traceback.print_exc()\n",
        "        depth_model_loader = None\n",
        "else:\n",
        "    print(\"Depth checkpoint missing/invalid.\")\n",
        "\n",
        "_depth_model = depth_model_loader # Assign to global\n",
        "\n",
        "# === YOLO Model Loading (from your OCR structure) ===\n",
        "YOLO_MODEL_NAME = 'yolov8s.pt'\n",
        "print(f\"\\nLoading YOLO model: {YOLO_MODEL_NAME}...\")\n",
        "yolo_model_loader = None # Local var\n",
        "try:\n",
        "    yolo_model_loader = YOLO(YOLO_MODEL_NAME)\n",
        "    print(f\"YOLO model '{YOLO_MODEL_NAME}' loaded.\")\n",
        "except Exception as e_yolo:\n",
        "    print(f\"!!! ERROR loading YOLO model: {e_yolo}\")\n",
        "_yolo_model = yolo_model_loader # Assign to global\n",
        "\n",
        "print(\"-\" * 70)\n",
        "if _depth_model: print(f\"✅ Depth Model '{MODEL_TYPE}' Ready.\")\n",
        "else: print(f\"❌ Depth Model Loading FAILED.\")\n",
        "if _yolo_model: print(f\"✅ YOLO Model '{YOLO_MODEL_NAME}' Ready.\")\n",
        "else: print(f\"❌ YOLO Model Loading FAILED.\")\n",
        "print(\"Global vars _depth_model, _yolo_model, MODEL_TYPE, DEVICE, model_configs should be set.\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc7lt1XEHPlD",
        "outputId": "0bd0ac80-982f-410d-e735-f1d794205078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cell 3: Helper function 'colorize_relative_depth' (original structure) defined.\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title 3. Helper Function for Colorizing Depth Map (Original Structure)\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "COLORMAP_VIS = cv2.COLORMAP_INFERNO # As per PDF Cell 6 config in OCR\n",
        "\n",
        "def colorize_relative_depth(raw_depth_map, colormap=COLORMAP_VIS): # This name matches your PDF's Cell 3\n",
        "    \"\"\"Applies a colormap to a raw depth map for visualization.\"\"\"\n",
        "    if raw_depth_map is None:\n",
        "        # print(\"[Warn] colorize_relative_depth received None input.\") # Original didn't print\n",
        "        return None # Return None if input is None\n",
        "    if not isinstance(raw_depth_map, np.ndarray):\n",
        "        # print(f\"[Warn] colorize_relative_depth received non-numpy input: {type(raw_depth_map)}\") # Original didn't print\n",
        "        return None\n",
        "\n",
        "    vis_map = raw_depth_map.astype(np.float32)\n",
        "    min_val = np.min(vis_map)\n",
        "    max_val = np.max(vis_map)\n",
        "\n",
        "    if max_val > min_val:\n",
        "        normalized = (vis_map - min_val) / (max_val - min_val)\n",
        "        depth_uint8 = (normalized * 255).astype(np.uint8)\n",
        "    else:\n",
        "        # print(\"[Warn] Depth map has zero range (min == max). Returning black image.\") # Original didn't print\n",
        "        depth_uint8 = np.zeros_like(vis_map, dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        colored_depth = cv2.applyColorMap(depth_uint8, colormap)\n",
        "        return colored_depth\n",
        "    except Exception as e:\n",
        "        # print(f\"[Error] Failed to apply colormap: {e}\") # Original didn't print\n",
        "        return None # Return None if colormapping fails (as per OCR page 9 logic)\n",
        "\n",
        "print(\"✅ Cell 3: Helper function 'colorize_relative_depth' (original structure) defined.\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "poXwJcRSHTd9",
        "outputId": "f521542a-707f-4ebf-c2f7-32857e8317d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload the video file you want to process (.mp4, .avi, etc.):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-caba9df5-e7b1-4cb9-8591-f1d11814f78c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-caba9df5-e7b1-4cb9-8591-f1d11814f78c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Generate_Crowded_Street_Scene_Video.mp4 to Generate_Crowded_Street_Scene_Video.mp4\n",
            "\n",
            "Uploaded 'Generate_Crowded_Street_Scene_Video.mp4' (6560482 bytes)\n",
            "Saved to: '/content/Generate_Crowded_Street_Scene_Video.mp4'\n",
            "\n",
            "✅ Cell 4: Video path set to '/content/Generate_Crowded_Street_Scene_Video.mp4'.\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title 4. Upload Video File for Streaming\n",
        "from google.colab import files # Specific to Google Colab\n",
        "import os\n",
        "# import time # time not used in OCR version of this cell\n",
        "\n",
        "# --- Check if running in Colab, otherwise skip file upload ---\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "input_video_path = None\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Please upload the video file you want to process (.mp4, .avi, etc.):\")\n",
        "    uploaded_video = files.upload()\n",
        "\n",
        "    if uploaded_video:\n",
        "        filename = list(uploaded_video.keys())[0]\n",
        "        save_dir = \"/content/\"\n",
        "        destination_path = os.path.join(save_dir, filename)\n",
        "        try:\n",
        "            with open(destination_path, 'wb') as f:\n",
        "                f.write(uploaded_video[filename])\n",
        "            print(f\"\\nUploaded '{filename}' ({len(uploaded_video[filename])} bytes)\")\n",
        "            print(f\"Saved to: '{destination_path}'\")\n",
        "            input_video_path = destination_path\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError saving uploaded file: {e}\")\n",
        "            input_video_path = None\n",
        "    else:\n",
        "        print(\"\\nNo video file was uploaded.\")\n",
        "        input_video_path = None # Explicitly None\n",
        "else:\n",
        "    print(\"Not running in Google Colab. Skipping interactive upload.\")\n",
        "    print(\"Please ensure 'input_video_path' is set manually if not using Colab upload.\")\n",
        "    # Example: input_video_path = \"my_local_video.mp4\"\n",
        "\n",
        "\n",
        "if input_video_path and os.path.exists(input_video_path):\n",
        "    print(f\"\\n✅ Cell 4: Video path set to '{input_video_path}'.\")\n",
        "elif IN_COLAB and not input_video_path : # Only relevant if in Colab and upload failed\n",
        "     print(f\"\\n❌ Cell 4: Video path NOT set in Colab. Upload may have failed or was skipped.\")\n",
        "else: # Covers not IN_COLAB and input_video_path not set manually\n",
        "    print(f\"\\n⚠️ Cell 4: Video path not set. If running locally, please set 'input_video_path' manually.\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "byYQkUyIKZpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47080b33-400b-44ee-c1a0-3f1483555b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cell 5: BEV with blinking/sized dots & trails (constants self-contained & checked).\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title 5. CORRECTED - Helper Functions & Configs (BEV Constants Scope for Dots & Trails)\n",
        "import cv2\n",
        "import numpy as np\n",
        "import logging\n",
        "from collections import deque # For object trails\n",
        "\n",
        "# --- Configuration ---\n",
        "COLOR_TEXT_BOX = (255, 255, 255)\n",
        "\n",
        "# --- BEV Configuration - DEFINED HERE TO BE IN SCOPE FOR create_enhanced_bev ---\n",
        "BEV_HEIGHT = 240; BEV_WIDTH = 320\n",
        "BEV_BACKGROUND_COLOR = (30, 30, 30) # Darker Background\n",
        "EGO_VEHICLE_COLOR_BEV = (0, 220, 0); EGO_VEHICLE_RECT_W = 20; EGO_VEHICLE_RECT_H = 12\n",
        "VEHICLE_POSITION_BEV = (BEV_WIDTH // 2, BEV_HEIGHT - 20) # Ego slightly higher\n",
        "\n",
        "# Dot Radii based on Danger Level\n",
        "BEV_DOT_RADIUS_DANGER = 8\n",
        "BEV_DOT_RADIUS_CAUTION = 6\n",
        "BEV_DOT_RADIUS_FAR = 4\n",
        "BEV_DOT_RADIUS_MIN = 2 # Explicitly define MIN for trails, must be >=1\n",
        "\n",
        "BEV_Y_SCALE_RAW_FRACTION = BEV_HEIGHT * 0.90 # Use more of the BEV height\n",
        "BEV_X_SCALE_FACTOR = BEV_WIDTH * 0.8\n",
        "BEV_DOT_BORDER_COLOR = (200, 200, 200); BEV_DOT_BORDER_THICKNESS = 1\n",
        "\n",
        "# Blinking Red Dots\n",
        "BLINK_FRAME_INTERVAL = 10\n",
        "\n",
        "# Object Trails\n",
        "MAX_TRAIL_LENGTH = 5\n",
        "BEV_OBJECT_TRACKS = {} # This will be reset in Cell 6's run_websocket_server\n",
        "NEXT_TRACK_ID = 0    # This will be reset in Cell 6's run_websocket_server\n",
        "TRACKING_MAX_DIST = 35 # Increased slightly\n",
        "\n",
        "# --- Constants for determining dot/box colors (mirroring PDF logic from Cell 6) ---\n",
        "PDF_COLOR_CLOSE = (0, 0, 255)    # Red\n",
        "PDF_COLOR_MEDIUM = (0, 255, 255) # Yellow\n",
        "PDF_COLOR_FAR = (0, 255, 0)      # Green\n",
        "\n",
        "PDF_RAW_VAL_CLOSE_THRESHOLD = 0.6 # Must match Cell 6 if PDF lines use it directly from there\n",
        "PDF_RAW_VAL_MEDIUM_THRESHOLD = 0.3\n",
        "\n",
        "# --- Other Constants for Enhancements ---\n",
        "ENHANCED_CONFIDENCE_THRESHOLD = 0.4\n",
        "ENHANCED_TARGET_CLASSES = []\n",
        "\n",
        "\n",
        "# get_raw_depth_at_center_for_enhancements (Same as before)\n",
        "def get_raw_depth_at_center_for_enhancements(raw_depth_map, box_coords, object_class_name=\"Obj\"):\n",
        "    if raw_depth_map is None: logging.debug(f\"Obj: {object_class_name} - Raw depth map None.\"); return None\n",
        "    x1,y1,x2,y2=map(int,box_coords); cx,cy=(x1+x2)/2,(y1+y2)/2\n",
        "    try:\n",
        "        h,w=raw_depth_map.shape[:2]; sy,sx=max(0,min(h-1,int(round(cy)))),max(0,min(w-1,int(round(cx))))\n",
        "        val=raw_depth_map[sy,sx]\n",
        "        if not np.isfinite(val): logging.debug(f\"Obj: {object_class_name} - Depth not finite: {val}\"); return None\n",
        "        return val\n",
        "    except Exception as e: logging.error(f\"Obj: {object_class_name} - Get depth error: {e}\"); return None\n",
        "\n",
        "# draw_ar_bounding_boxes (Same as before - uses PDF_COLOR_* defined above in this cell)\n",
        "def draw_ar_bounding_boxes(frame_to_draw_on, yolo_results,\n",
        "                           raw_depth_map_for_color, current_frame_min_raw, current_frame_max_raw,\n",
        "                           yolo_model_names):\n",
        "    if yolo_results is None or len(yolo_results) == 0 or yolo_results[0].boxes is None: return frame_to_draw_on\n",
        "    boxes_data = yolo_results[0].boxes\n",
        "    for i in range(len(boxes_data.xyxy)):\n",
        "        box=boxes_data.xyxy[i].cpu().numpy(); conf=boxes_data.conf[i].cpu().numpy()\n",
        "        cls_id=int(boxes_data.cls[i].cpu().numpy()); class_name=yolo_model_names[cls_id]\n",
        "        if ENHANCED_TARGET_CLASSES and class_name not in ENHANCED_TARGET_CLASSES: continue\n",
        "        if conf < ENHANCED_CONFIDENCE_THRESHOLD: continue\n",
        "        x1,y1,x2,y2=map(int,box); obj_raw_depth=None\n",
        "        if raw_depth_map_for_color is not None: obj_raw_depth=get_raw_depth_at_center_for_enhancements(raw_depth_map_for_color,box,class_name)\n",
        "        box_color=PDF_COLOR_FAR\n",
        "        depth_text=\"(Depth N/A)\"\n",
        "        if obj_raw_depth is not None and (current_frame_max_raw > current_frame_min_raw):\n",
        "            depth_text=f\"({obj_raw_depth:.2e})\"\n",
        "            d_range=current_frame_max_raw-current_frame_min_raw\n",
        "            close_thr=current_frame_min_raw+d_range*PDF_RAW_VAL_CLOSE_THRESHOLD\n",
        "            med_thr=current_frame_min_raw+d_range*PDF_RAW_VAL_MEDIUM_THRESHOLD\n",
        "            if obj_raw_depth>=close_thr: box_color=PDF_COLOR_CLOSE\n",
        "            elif obj_raw_depth>=med_thr: box_color=PDF_COLOR_MEDIUM\n",
        "        txt=f\"{class_name}({conf:.2f}){depth_text}\"; cv2.rectangle(frame_to_draw_on,(x1,y1),(x2,y2),box_color,2)\n",
        "        (tw,th),_=cv2.getTextSize(txt,cv2.FONT_HERSHEY_SIMPLEX,0.5,1)\n",
        "        tbg_y1=max(0,y1-th-4); ty_pos=max(th,y1-4)\n",
        "        cv2.rectangle(frame_to_draw_on,(x1,tbg_y1),(x1+tw,y1),box_color,-1)\n",
        "        cv2.putText(frame_to_draw_on,txt,(x1,ty_pos),cv2.FONT_HERSHEY_SIMPLEX,0.5,COLOR_TEXT_BOX,1,cv2.LINE_AA)\n",
        "    return frame_to_draw_on\n",
        "\n",
        "def update_bev_tracks(current_detections_bev): # Same as before\n",
        "    global BEV_OBJECT_TRACKS, NEXT_TRACK_ID\n",
        "    if not BEV_OBJECT_TRACKS:\n",
        "        for det in current_detections_bev:\n",
        "            BEV_OBJECT_TRACKS[NEXT_TRACK_ID] = deque([ (det['x_base'], det['y_base'], det['color']) ], maxlen=MAX_TRAIL_LENGTH)\n",
        "            det['track_id'] = NEXT_TRACK_ID; NEXT_TRACK_ID += 1\n",
        "        return current_detections_bev\n",
        "    unmatched_track_ids = list(BEV_OBJECT_TRACKS.keys()); unmatched_detections = list(range(len(current_detections_bev)))\n",
        "    for track_id in list(unmatched_track_ids):\n",
        "        last_x, last_y, _ = BEV_OBJECT_TRACKS[track_id][-1]; best_dist = float('inf'); best_det_idx = -1\n",
        "        for det_idx in list(unmatched_detections):\n",
        "            det = current_detections_bev[det_idx]; dist = np.sqrt((last_x - det['x_base'])**2 + (last_y - det['y_base'])**2)\n",
        "            if dist < TRACKING_MAX_DIST and dist < best_dist: best_dist = dist; best_det_idx = det_idx\n",
        "        if best_det_idx != -1:\n",
        "            matched_det = current_detections_bev[best_det_idx]\n",
        "            BEV_OBJECT_TRACKS[track_id].append( (matched_det['x_base'], matched_det['y_base'], matched_det['color']) )\n",
        "            matched_det['track_id'] = track_id; unmatched_track_ids.remove(track_id); unmatched_detections.remove(best_det_idx)\n",
        "    for track_id in unmatched_track_ids: del BEV_OBJECT_TRACKS[track_id]\n",
        "    for det_idx in unmatched_detections:\n",
        "        det = current_detections_bev[det_idx]\n",
        "        BEV_OBJECT_TRACKS[NEXT_TRACK_ID] = deque([ (det['x_base'], det['y_base'], det['color']) ], maxlen=MAX_TRAIL_LENGTH)\n",
        "        det['track_id'] = NEXT_TRACK_ID; NEXT_TRACK_ID += 1\n",
        "    return current_detections_bev\n",
        "\n",
        "def create_enhanced_bev(yolo_results,\n",
        "                        raw_depth_map_for_color, current_frame_min_raw, current_frame_max_raw,\n",
        "                        yolo_model_names, original_frame_width, current_frame_number):\n",
        "    global BEV_OBJECT_TRACKS\n",
        "    bev_image = np.full((BEV_HEIGHT, BEV_WIDTH, 3), BEV_BACKGROUND_COLOR, dtype=np.uint8)\n",
        "    ego_center_x, ego_center_y = VEHICLE_POSITION_BEV\n",
        "\n",
        "    # Draw subtle grid (as before)\n",
        "    for i in range(1, 6):\n",
        "        line_y = ego_center_y - int( (BEV_HEIGHT * 0.9 / 5) * i )\n",
        "        if line_y < 0: break\n",
        "        cv2.line(bev_image, (0, line_y), (BEV_WIDTH, line_y), (60,60,60), 1)\n",
        "    # ... (rest of grid drawing) ...\n",
        "\n",
        "    cv2.rectangle(bev_image, (ego_center_x - EGO_VEHICLE_RECT_W//2, ego_center_y - EGO_VEHICLE_RECT_H//2),\n",
        "                  (ego_center_x + EGO_VEHICLE_RECT_W//2, ego_center_y + EGO_VEHICLE_RECT_H//2), EGO_VEHICLE_COLOR_BEV, -1)\n",
        "    cv2.line(bev_image, (ego_center_x, ego_center_y - EGO_VEHICLE_RECT_H//2), (ego_center_x, ego_center_y - EGO_VEHICLE_RECT_H//2 - 6), EGO_VEHICLE_COLOR_BEV, 3)\n",
        "\n",
        "    current_detections_for_tracking = []\n",
        "    if not (yolo_results and len(yolo_results) > 0 and yolo_results[0].boxes and raw_depth_map_for_color is not None):\n",
        "        for track_id, trail in list(BEV_OBJECT_TRACKS.items()):\n",
        "            for i, (tx, ty, tcolor) in enumerate(trail):\n",
        "                alpha_trail = 1.0 - ( (len(trail) - 1 - i) / MAX_TRAIL_LENGTH )\n",
        "                trail_dot_color = (int(tcolor[0]*alpha_trail), int(tcolor[1]*alpha_trail), int(tcolor[2]*alpha_trail))\n",
        "                # Use BEV_DOT_RADIUS_MIN defined at the top of this cell\n",
        "                cv2.circle(bev_image, (tx, ty), max(1, BEV_DOT_RADIUS_MIN -1 if BEV_DOT_RADIUS_MIN > 1 else 1), trail_dot_color, -1)\n",
        "        # ... (Legend drawing) ...\n",
        "        return bev_image\n",
        "\n",
        "    boxes_data = yolo_results[0].boxes\n",
        "    for i in range(len(boxes_data.xyxy)):\n",
        "        box=boxes_data.xyxy[i].cpu().numpy(); conf=boxes_data.conf[i].cpu().numpy()\n",
        "        cls_id=int(boxes_data.cls[i].cpu().numpy()); class_name=yolo_model_names[cls_id]\n",
        "        if ENHANCED_TARGET_CLASSES and class_name not in ENHANCED_TARGET_CLASSES: continue\n",
        "        if conf < ENHANCED_CONFIDENCE_THRESHOLD: continue\n",
        "        x1_obj, _, x2_obj, _ = map(int, box); center_x_orig_obj = (x1_obj + x2_obj) // 2\n",
        "        object_raw_depth_value = get_raw_depth_at_center_for_enhancements(raw_depth_map_for_color, box, class_name)\n",
        "\n",
        "        dot_color = PDF_COLOR_FAR # Uses PDF_COLOR_FAR from this cell's top\n",
        "        dot_radius = BEV_DOT_RADIUS_FAR # Uses BEV_DOT_RADIUS_FAR from this cell's top\n",
        "        bev_y_fraction_from_ego = 1.0\n",
        "\n",
        "        if object_raw_depth_value is not None and (current_frame_max_raw > current_frame_min_raw):\n",
        "            depth_range_frame = current_frame_max_raw - current_frame_min_raw\n",
        "            close_thresh_val = current_frame_min_raw + depth_range_frame * PDF_RAW_VAL_CLOSE_THRESHOLD\n",
        "            medium_thresh_val = current_frame_min_raw + depth_range_frame * PDF_RAW_VAL_MEDIUM_THRESHOLD\n",
        "            if object_raw_depth_value >= close_thresh_val:\n",
        "                dot_color = PDF_COLOR_CLOSE; dot_radius = BEV_DOT_RADIUS_DANGER\n",
        "            elif object_raw_depth_value >= medium_thresh_val:\n",
        "                dot_color = PDF_COLOR_MEDIUM; dot_radius = BEV_DOT_RADIUS_CAUTION\n",
        "            clamped_depth = np.clip(object_raw_depth_value, current_frame_min_raw, current_frame_max_raw)\n",
        "            if depth_range_frame > 1e-5: bev_y_fraction_from_ego = (current_frame_max_raw - clamped_depth) / depth_range_frame\n",
        "            else: bev_y_fraction_from_ego = 0.0\n",
        "\n",
        "        y_base = ego_center_y - int(bev_y_fraction_from_ego * BEV_Y_SCALE_RAW_FRACTION)\n",
        "        perspective_x_factor = 1.0 - (0.4 * bev_y_fraction_from_ego)\n",
        "        x_offset_from_center = ((center_x_orig_obj / original_frame_width) - 0.5) * BEV_X_SCALE_FACTOR * perspective_x_factor\n",
        "        x_base = ego_center_x + int(x_offset_from_center)\n",
        "        x_base = np.clip(x_base,0,BEV_WIDTH-1); y_base = np.clip(y_base, BEV_HORIZON_Y if 'BEV_HORIZON_Y' in globals() else 0, BEV_HEIGHT-1) # Use BEV_HORIZON_Y if defined\n",
        "\n",
        "        current_detections_for_tracking.append({'x_base': x_base, 'y_base': y_base,\n",
        "                                                'color': dot_color, 'radius': dot_radius,\n",
        "                                                'raw_depth': object_raw_depth_value, 'class_name': class_name})\n",
        "\n",
        "    update_bev_tracks(current_detections_for_tracking)\n",
        "\n",
        "    for track_id, trail in list(BEV_OBJECT_TRACKS.items()):\n",
        "        for i_trail, (tx_base, ty_base, tcolor) in enumerate(trail):\n",
        "            if i_trail < len(trail) - 1:\n",
        "                alpha_trail = 0.1 + 0.9 * (i_trail / MAX_TRAIL_LENGTH)\n",
        "                trail_dot_color = (int(tcolor[0]*alpha_trail), int(tcolor[1]*alpha_trail), int(tcolor[2]*alpha_trail))\n",
        "                # Use BEV_DOT_RADIUS_MIN defined at the top of this cell\n",
        "                trail_radius = BEV_DOT_RADIUS_MIN - 2 + int((i_trail / MAX_TRAIL_LENGTH) * 2) # Ensure trail_radius >= 1\n",
        "                cv2.circle(bev_image, (tx_base, ty_base), max(1, trail_radius), trail_dot_color, -1)\n",
        "\n",
        "        current_x_base, current_y_base, current_color = trail[-1]\n",
        "        current_radius = BEV_DOT_RADIUS_FAR # Uses BEV_DOT_RADIUS_* from this cell\n",
        "        if current_color == PDF_COLOR_CLOSE: current_radius = BEV_DOT_RADIUS_DANGER\n",
        "        elif current_color == PDF_COLOR_MEDIUM: current_radius = BEV_DOT_RADIUS_CAUTION\n",
        "\n",
        "        dot_is_visible = True\n",
        "        if current_color == PDF_COLOR_CLOSE: # Uses PDF_COLOR_CLOSE from this cell\n",
        "            if (current_frame_number // (BLINK_FRAME_INTERVAL // 2)) % 2 == 0: dot_is_visible = False # Uses BLINK_FRAME_INTERVAL\n",
        "\n",
        "        if dot_is_visible:\n",
        "            cv2.circle(bev_image, (current_x_base, current_y_base), current_radius, current_color, -1)\n",
        "            cv2.circle(bev_image, (current_x_base, current_y_base), current_radius, BEV_DOT_BORDER_COLOR, BEV_DOT_BORDER_THICKNESS)\n",
        "            object_bev_height_px = BEV_OBJECT_DEFAULT_HEIGHT_PX if 'BEV_OBJECT_DEFAULT_HEIGHT_PX' in globals() else 4 # Use if defined\n",
        "            display_height_px = object_bev_height_px\n",
        "            if current_color == PDF_COLOR_CLOSE: display_height_px = int(object_bev_height_px * 1.5)\n",
        "            elif current_color == PDF_COLOR_MEDIUM: display_height_px = int(object_bev_height_px * 1.2)\n",
        "            y_top = max(BEV_HORIZON_Y if 'BEV_HORIZON_Y' in globals() else 0, current_y_base - display_height_px)\n",
        "            cv2.line(bev_image, (current_x_base, current_y_base), (current_x_base, y_top), current_color, max(1, current_radius // 2))\n",
        "\n",
        "    # Legend uses PDF_COLOR_* from this cell\n",
        "    cv2.putText(bev_image,\"BEV Zones:\",(5,15),cv2.FONT_HERSHEY_SIMPLEX,0.4,COLOR_TEXT_BOX,1)\n",
        "    cv2.circle(bev_image,(10,30),5,PDF_COLOR_CLOSE,-1); cv2.line(bev_image,(10,30),(10,30),BEV_DOT_BORDER_COLOR,1); cv2.putText(bev_image,\"Close\",(20,35),cv2.FONT_HERSHEY_SIMPLEX,0.4,COLOR_TEXT_BOX,1)\n",
        "    cv2.circle(bev_image,(10,50),5,PDF_COLOR_MEDIUM,-1); cv2.line(bev_image,(10,50),(10,50),BEV_DOT_BORDER_COLOR,1); cv2.putText(bev_image,\"Medium\",(20,55),cv2.FONT_HERSHEY_SIMPLEX,0.4,COLOR_TEXT_BOX,1)\n",
        "    cv2.circle(bev_image,(10,70),5,PDF_COLOR_FAR,-1); cv2.line(bev_image,(10,70),(10,70),BEV_DOT_BORDER_COLOR,1); cv2.putText(bev_image,\"Far\",(20,75),cv2.FONT_HERSHEY_SIMPLEX,0.4,COLOR_TEXT_BOX,1)\n",
        "    return bev_image\n",
        "\n",
        "print(\"✅ Cell 5: BEV with blinking/sized dots & trails (constants self-contained & checked).\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-ntfAENFLmHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f770c6f2-c392-4d76-b22a-2977d579749d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cell 6: WebSocket Server (BEV with Blinking/Sized Dots & Trails).\n",
            "   Set LOGGING_LEVEL. ENABLE_VIDEO_WRITING to save.\n",
            "   Run 'run_websocket_server()' in Cell 7 to start.\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title 6. WebSocket Server Definitions (BEV with Blinking/Sized Dots & Trails)\n",
        "import asyncio\n",
        "import websockets\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import traceback\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import logging\n",
        "from google.colab import files # For download option in Colab\n",
        "# from collections import deque # deque is used in Cell 5's update_bev_tracks\n",
        "\n",
        "# --- Logging Configuration ---\n",
        "LOGGING_LEVEL = logging.INFO # Change to DEBUG for very verbose logs\n",
        "logging.basicConfig(\n",
        "    level=LOGGING_LEVEL,\n",
        "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "try:\n",
        "    nest_asyncio.apply()\n",
        "    logging.info(\"nest_asyncio applied.\")\n",
        "except RuntimeError:\n",
        "    logging.warning(\"nest_asyncio already applied or cannot be applied.\")\n",
        "\n",
        "# --- WebSocket and PDF Line Drawing Constants ---\n",
        "WEBSOCKET_PORT = 8765\n",
        "active_connections = set()\n",
        "PDF_CONFIDENCE_THRESHOLD = 0.4\n",
        "PDF_TARGET_CLASSES = [] # From your PDF, not used by enhancements unless ENHANCED_TARGET_CLASSES is set\n",
        "PDF_LINE_THICKNESS = 2\n",
        "PDF_RAW_VAL_CLOSE_THRESHOLD = 0.6\n",
        "PDF_RAW_VAL_MEDIUM_THRESHOLD = 0.3\n",
        "PDF_COLOR_CLOSE = (0, 0, 255)    # Red\n",
        "PDF_COLOR_MEDIUM = (0, 255, 255) # Yellow\n",
        "PDF_COLOR_FAR = (0, 255, 0)      # Green\n",
        "\n",
        "# --- Model Input Size for Depth Model (from PDF Cell 15) ---\n",
        "model_input_size = 518\n",
        "\n",
        "# --- Video Output Configuration ---\n",
        "OUTPUT_VIDEO_FILENAME = \"processed_video_output.mp4\"\n",
        "ENABLE_VIDEO_WRITING = True # Set to False to disable writing video to file\n",
        "output_video_writer = None # Global VideoWriter object\n",
        "\n",
        "# --- Helper: Get Depth at Point (for PDF Line Logic - from PDF Cell 12) ---\n",
        "def get_depth_at_point_pdf(raw_depth_map_input, x, y):\n",
        "    if raw_depth_map_input is None: return None\n",
        "    try:\n",
        "        h, w = raw_depth_map_input.shape[:2]\n",
        "        safe_y = max(0, min(h - 1, int(round(y))))\n",
        "        safe_x = max(0, min(w - 1, int(round(x))))\n",
        "        val = raw_depth_map_input[safe_y, safe_x]\n",
        "        return val if np.isfinite(val) else None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Err PDF depth @({x},{y}): {e}\", exc_info=False)\n",
        "        return None\n",
        "\n",
        "# --- Helper: Draw Colored Depth Line (for PDF Line Logic - from PDF Cell 12-13) ---\n",
        "def draw_colored_relative_depth_line_pdf(image, obj_center_x, obj_center_y, object_raw_depth_value, frame_min_raw, frame_max_raw):\n",
        "    if object_raw_depth_value is None or not (frame_max_raw > frame_min_raw) : return\n",
        "    h, w = image.shape[:2]; cam_ox, cam_oy = w // 2, h - 1\n",
        "    depth_range = frame_max_raw - frame_min_raw\n",
        "    close_thresh = frame_min_raw + depth_range * PDF_RAW_VAL_CLOSE_THRESHOLD\n",
        "    medium_thresh = frame_min_raw + depth_range * PDF_RAW_VAL_MEDIUM_THRESHOLD\n",
        "    line_color = PDF_COLOR_FAR\n",
        "    if object_raw_depth_value >= close_thresh: line_color = PDF_COLOR_CLOSE\n",
        "    elif object_raw_depth_value >= medium_thresh: line_color = PDF_COLOR_MEDIUM\n",
        "    try:\n",
        "        cv2.line(image, (cam_ox, cam_oy), (int(round(obj_center_x)), int(round(obj_center_y))), line_color, PDF_LINE_THICKNESS)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[PDF Line Err] Draw: {e}\", exc_info=False)\n",
        "\n",
        "\n",
        "async def video_stream_server(websocket):\n",
        "    global _depth_model, _yolo_model, DEVICE, _confirmed_video_path, active_connections\n",
        "    global MODEL_TYPE, ENHANCED_CONFIDENCE_THRESHOLD # ENHANCED_CONFIDENCE_THRESHOLD is from Cell 5\n",
        "    global output_video_writer, ENABLE_VIDEO_WRITING, OUTPUT_VIDEO_FILENAME\n",
        "    # PREVIOUS_DEPTH_MAP_SMOOTH is also global if used by Cell 5's BEV or other logic if re-enabled\n",
        "    # For this version, direct raw_depth_map is used for BEV coloring consistency.\n",
        "\n",
        "    connection_id = websocket.remote_address if websocket.remote_address else f\"UnknownClient-{time.time():.0f}\"\n",
        "    active_connections.add(websocket)\n",
        "    logging.info(f\"Client connected: {connection_id}. Total: {len(active_connections)}\")\n",
        "\n",
        "    if not all([_depth_model, _yolo_model, _confirmed_video_path, MODEL_TYPE]):\n",
        "        reason = \"Server error: Missing critical component(s).\"\n",
        "        logging.error(f\"[{connection_id}] Pre-check failed: {reason}\")\n",
        "        await websocket.close(code=1011, reason=reason.strip()); active_connections.discard(websocket); return\n",
        "\n",
        "    video_path_to_use = _confirmed_video_path\n",
        "    cap = cv2.VideoCapture(video_path_to_use)\n",
        "    if not cap.isOpened():\n",
        "        logging.error(f\"[{connection_id}] Error: Cannot open video: {video_path_to_use}\")\n",
        "        await websocket.close(code=1011, reason=\"Server error: Cannot open video file.\"); active_connections.discard(websocket); return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS); w_vid = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h_vid = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    logging.info(f\"[{connection_id}] Vid: {w_vid}x{h_vid} @ {fps:.1f}FPS. DepthM: {MODEL_TYPE}, Enh.YOLO Conf: {ENHANCED_CONFIDENCE_THRESHOLD}, PDF.YOLO Conf: {PDF_CONFIDENCE_THRESHOLD}\")\n",
        "\n",
        "    if ENABLE_VIDEO_WRITING and output_video_writer is None:\n",
        "        preview_h_vid = max(120, h_vid // 3); combined_h = h_vid + preview_h_vid\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v'); out_path = os.path.join(\"/content/\", OUTPUT_VIDEO_FILENAME)\n",
        "        try:\n",
        "            output_video_writer = cv2.VideoWriter(out_path, fourcc, fps if fps > 0 else 10, (w_vid, combined_h))\n",
        "            if output_video_writer.isOpened(): logging.info(f\"Output video: {out_path} @ {w_vid}x{combined_h}\")\n",
        "            else: logging.error(f\"Failed to open VideoWriter for {out_path}\"); output_video_writer = None\n",
        "        except Exception as e_vw: logging.error(f\"VideoWriter Exception: {e_vw}\"); output_video_writer = None\n",
        "\n",
        "    frame_count = 0; timers = {k:0 for k in [\"depth\",\"yolo\",\"draw_pdf\",\"draw_enh\",\"encode\",\"send\",\"loop_proc\"]}\n",
        "    start_process_time = time.time()\n",
        "\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            loop_s = time.time(); ret, frame = cap.read(); frame_count += 1\n",
        "            if not ret or frame is None: logging.info(f\"End of video or bad frame {frame_count}.\"); break\n",
        "            output_frame = frame.copy()\n",
        "\n",
        "            depth_s = time.time(); depth_map_raw = None\n",
        "            try:\n",
        "                with torch.no_grad(): depth_map_raw = _depth_model.infer_image(frame, input_size=model_input_size)\n",
        "                if depth_map_raw is None: logging.error(f\"!!!! Fr{frame_count}: _depth_model.infer_image returned None !!!!\")\n",
        "                else: logging.debug(f\"Fr{frame_count}: Raw depth OK. Shape:{depth_map_raw.shape}\")\n",
        "            except Exception as e: logging.error(f\"!!!! Fr{frame_count}: EXCEPTION depth inference: {e} !!!!\", exc_info=LOGGING_LEVEL <= logging.DEBUG); depth_map_raw = None\n",
        "            timers[\"depth\"] += time.time() - depth_s\n",
        "\n",
        "            yolo_s = time.time(); yolo_results = None\n",
        "            try: yolo_results = _yolo_model(frame, device=DEVICE, verbose=False, conf=min(PDF_CONFIDENCE_THRESHOLD, ENHANCED_CONFIDENCE_THRESHOLD))\n",
        "            except Exception as e: logging.error(f\"Fr{frame_count}: YOLO Err: {e}\", exc_info=LOGGING_LEVEL <= logging.DEBUG)\n",
        "            timers[\"yolo\"] += time.time() - yolo_s\n",
        "\n",
        "            current_frame_min_raw, current_frame_max_raw = 0, 1\n",
        "            if depth_map_raw is not None:\n",
        "                finite_map = depth_map_raw[np.isfinite(depth_map_raw)]\n",
        "                if finite_map.size > 0:\n",
        "                    current_frame_min_raw = np.min(finite_map)\n",
        "                    current_frame_max_raw = np.max(finite_map)\n",
        "\n",
        "            pdf_draw_s = time.time()\n",
        "            if depth_map_raw is not None and (current_frame_max_raw > current_frame_min_raw) and yolo_results and len(yolo_results) > 0 and yolo_results[0].boxes is not None:\n",
        "                 boxes_pdf = yolo_results[0].boxes\n",
        "                 for i in range(len(boxes_pdf.xyxy)):\n",
        "                    if boxes_pdf.conf[i] < PDF_CONFIDENCE_THRESHOLD: continue\n",
        "                    x1p,y1p,x2p,y2p=map(int,boxes_pdf.xyxy[i].cpu().numpy()); cxp,cyp=(x1p+x2p)/2,(y1p+y2p)/2\n",
        "                    depth_val_line = get_depth_at_point_pdf(depth_map_raw, cxp, cyp)\n",
        "                    draw_colored_relative_depth_line_pdf(output_frame, cxp, cyp, depth_val_line, current_frame_min_raw, current_frame_max_raw)\n",
        "            timers[\"draw_pdf\"] += time.time() - pdf_draw_s\n",
        "\n",
        "            enh_draw_s = time.time()\n",
        "            output_frame_with_enhancements = draw_ar_bounding_boxes(output_frame, yolo_results,\n",
        "                                                                    depth_map_raw, current_frame_min_raw, current_frame_max_raw,\n",
        "                                                                    _yolo_model.names)\n",
        "            # Pass frame_count to create_enhanced_bev for blinking logic\n",
        "            bev_display = create_enhanced_bev(yolo_results,\n",
        "                                              depth_map_raw, current_frame_min_raw, current_frame_max_raw,\n",
        "                                              _yolo_model.names, w_vid, frame_count) # Added frame_count\n",
        "            timers[\"draw_enh\"] += time.time() - enh_draw_s\n",
        "\n",
        "            depth_colored_display = colorize_relative_depth(depth_map_raw)\n",
        "            if depth_colored_display is None:\n",
        "                depth_colored_display = np.zeros((h_vid, w_vid, 3), dtype=np.uint8)\n",
        "                cv2.putText(depth_colored_display, \"Depth Fail\" if depth_map_raw is None else \"Colorize Fail\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255),2)\n",
        "            elif depth_colored_display.shape[:2] != (h_vid,w_vid): depth_colored_display = cv2.resize(depth_colored_display, (w_vid,h_vid))\n",
        "\n",
        "            preview_h = max(120, h_vid // 3); preview_w_half = w_vid // 2\n",
        "            bev_preview = cv2.resize(bev_display, (preview_w_half, preview_h), interpolation=cv2.INTER_AREA)\n",
        "            depth_preview = cv2.resize(depth_colored_display, (preview_w_half, preview_h), interpolation=cv2.INTER_AREA)\n",
        "            aux_row = np.hstack((bev_preview, depth_preview))\n",
        "            if aux_row.shape[1] != w_vid: aux_row = cv2.resize(aux_row, (w_vid, preview_h), interpolation=cv2.INTER_AREA)\n",
        "            final_display = np.vstack((output_frame_with_enhancements, aux_row))\n",
        "\n",
        "            if ENABLE_VIDEO_WRITING and output_video_writer is not None and output_video_writer.isOpened():\n",
        "                writer_expected_h = h_vid + preview_h\n",
        "                if final_display.shape[0] != writer_expected_h or final_display.shape[1] != w_vid:\n",
        "                    final_display_for_write = cv2.resize(final_display, (w_vid, writer_expected_h))\n",
        "                    output_video_writer.write(final_display_for_write)\n",
        "                else: output_video_writer.write(final_display)\n",
        "\n",
        "            encode_s = time.time(); ret_enc, buffer = cv2.imencode('.jpg', final_display, [int(cv2.IMWRITE_JPEG_QUALITY), 75])\n",
        "            timers[\"encode\"] += time.time() - encode_s;\n",
        "            if not ret_enc: logging.warning(f\"JPEG encode fail fr {frame_count}\"); continue\n",
        "            send_s = time.time(); await websocket.send(buffer.tobytes()); timers[\"send\"] += time.time() - send_s\n",
        "            timers[\"loop_proc\"] += time.time() - loop_s\n",
        "            await asyncio.sleep(0.001)\n",
        "\n",
        "    except websockets.exceptions.ConnectionClosed: logging.info(f\"[{connection_id}] Client disconnected.\")\n",
        "    except websockets.exceptions.ConnectionClosedOK: logging.info(f\"[{connection_id}] Client disconnected normally.\")\n",
        "    except Exception as e: logging.error(f\"!!! UNEXPECTED ERROR for {connection_id}: {e} !!!\", exc_info=True); traceback.print_exc()\n",
        "    finally:\n",
        "        duration = time.time() - start_process_time; cap.release(); active_connections.discard(websocket)\n",
        "        # PREVIOUS_DEPTH_MAP_SMOOTH is defined in Cell 5, ensure it's handled if used\n",
        "        if 'PREVIOUS_DEPTH_MAP_SMOOTH' in globals() and not active_connections: globals()['PREVIOUS_DEPTH_MAP_SMOOTH'] = None\n",
        "\n",
        "        if not active_connections and output_video_writer is not None:\n",
        "            if output_video_writer.isOpened(): logging.info(f\"Finalizing video: {OUTPUT_VIDEO_FILENAME}\"); output_video_writer.release()\n",
        "            output_video_writer = None\n",
        "            if 'google.colab' in sys.modules:\n",
        "                try:\n",
        "                    out_path_final = os.path.join(\"/content/\", OUTPUT_VIDEO_FILENAME)\n",
        "                    if os.path.exists(out_path_final): print(f\"Offering download: {out_path_final}\"); files.download(out_path_final)\n",
        "                except Exception as e_dl: print(f\"Download err: {e_dl}\")\n",
        "        logging.info(f\"[{connection_id}] Closed. Processed {frame_count} frames in {duration:.2f}s.\")\n",
        "        if frame_count > 0:\n",
        "             log_msg = f\"AvgTimes(ms/fr) for {connection_id}: \"; timers_list = [(k,v) for k,v in timers.items()]; log_msg += \" | \".join([f\"{k}={(v/frame_count)*1000:.1f}\" for k,v in timers_list]); logging.info(log_msg)\n",
        "        logging.info(f\"Remaining clients: {len(active_connections)}\")\n",
        "\n",
        "async def start_server():\n",
        "    global WEBSOCKET_PORT, NGROK_AUTH_TOKEN, active_connections, _depth_model, _yolo_model, _confirmed_video_path, MODEL_TYPE, output_video_writer\n",
        "    http_tunnel=None; video_path_exists = False\n",
        "    if _confirmed_video_path and isinstance(_confirmed_video_path, str): video_path_exists = os.path.exists(_confirmed_video_path)\n",
        "    if not all([_depth_model, _yolo_model, _confirmed_video_path, MODEL_TYPE, video_path_exists]): logging.error(\"!!! ABORT: Critical components not ready for server start.\"); return\n",
        "    ws_url = f\"ws://localhost:{WEBSOCKET_PORT}\"\n",
        "    # Ensure NGROK_AUTH_TOKEN is checked against your actual placeholder string from Cell 1 (e.g. \"YOUR_ACTUAL_NGROK_AUTH_TOKEN_HERE\")\n",
        "    if NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != \"YOUR_ACTUAL_NGROK_AUTH_TOKEN_HERE\" and NGROK_AUTH_TOKEN.strip() != \"\":\n",
        "        try:\n",
        "            for t in ngrok.get_tunnels():\n",
        "                if str(t.config.get('addr','')).endswith(str(WEBSOCKET_PORT)): ngrok.disconnect(t.public_url); await asyncio.sleep(1)\n",
        "            http_tunnel=ngrok.connect(WEBSOCKET_PORT,\"http\",bind_tls=True); p_url=http_tunnel.public_url\n",
        "            ws_url = p_url.replace(\"https://\",\"wss://\") if p_url.startswith(\"https\") else p_url.replace(\"http://\",\"ws://\")\n",
        "            logging.info(f\"Ngrok tunnel: {p_url} (HTTP), {ws_url} (WS/WSS)\")\n",
        "        except Exception as e: logging.error(f\"NGROK FAIL: {e}\", exc_info=True); logging.warning(\"Ngrok failed. Local only.\")\n",
        "    else: logging.warning(\"Ngrok Auth Token not set/placeholder. Skipping ngrok.\")\n",
        "    print(f\"\\n{'='*70}\\n=== WS Server Ready ===\\n=== Connect client to: {ws_url} ===\\n{'='*70}\\n\")\n",
        "    server_inst=None\n",
        "    try:\n",
        "        server_inst = await websockets.serve(video_stream_server, \"0.0.0.0\", WEBSOCKET_PORT, ping_interval=20, ping_timeout=20, max_size=15*1024*1024)\n",
        "        logging.info(f\"WebSocket server on 0.0.0.0:{WEBSOCKET_PORT}. Waiting for connections...\"); await asyncio.Future()\n",
        "    except Exception as e: logging.error(f\"WS SERVER Error on startup: {e}\", exc_info=True)\n",
        "    finally:\n",
        "        logging.info(\"Initiating server shutdown sequence...\")\n",
        "        if server_inst: server_inst.close(); await server_inst.wait_closed(); logging.info(\"WS server stopped.\")\n",
        "        if output_video_writer is not None:\n",
        "            if output_video_writer.isOpened(): logging.info(\"Releasing video writer (server stop)...\"); output_video_writer.release()\n",
        "            output_video_writer = None\n",
        "            if 'google.colab' in sys.modules:\n",
        "                out_path_final = os.path.join(\"/content/\", OUTPUT_VIDEO_FILENAME)\n",
        "                if os.path.exists(out_path_final): print(f\"Offering download (server stop): {out_path_final}\"); files.download(out_path_final)\n",
        "        if http_tunnel:\n",
        "          try: ngrok.disconnect(http_tunnel.public_url); ngrok.kill(); logging.info(\"Ngrok stopped.\")\n",
        "          except Exception as e_ngrok: logging.warning(f\"Ngrok disconnect error: {e_ngrok}\")\n",
        "        active_connections.clear(); logging.info(\"Server shutdown complete.\")\n",
        "\n",
        "def run_websocket_server():\n",
        "    global _confirmed_video_path, _depth_model, _yolo_model, MODEL_TYPE\n",
        "    global BEV_OBJECT_TRACKS, NEXT_TRACK_ID # For resetting tracks\n",
        "\n",
        "    # Reset BEV tracks each time server runs\n",
        "    BEV_OBJECT_TRACKS = {}\n",
        "    NEXT_TRACK_ID = 0\n",
        "\n",
        "    # Removed preload_icons as we are using dots\n",
        "    logging.info(\"BEV will use colored dots (icons are not preloaded in this version).\")\n",
        "\n",
        "    if 'input_video_path' in globals() and globals()['input_video_path']: _confirmed_video_path = globals()['input_video_path']\n",
        "    elif '_confirmed_video_path' not in globals() or not globals()['_confirmed_video_path']: _confirmed_video_path = None\n",
        "\n",
        "    path_ok = False;\n",
        "    if _confirmed_video_path and isinstance(_confirmed_video_path, str): path_ok = os.path.exists(_confirmed_video_path)\n",
        "    model_type_ok = 'MODEL_TYPE' in globals() and globals()['MODEL_TYPE'] is not None\n",
        "    depth_ok = '_depth_model' in globals() and globals()['_depth_model'] is not None\n",
        "    yolo_ok = '_yolo_model' in globals() and globals()['_yolo_model'] is not None\n",
        "\n",
        "    if path_ok and depth_ok and yolo_ok and model_type_ok:\n",
        "        logging.info(\"All prerequisites met. Starting server event loop...\");\n",
        "        asyncio.run(start_server())\n",
        "    else:\n",
        "        logging.error(\"Prerequisites not met. Server startup aborted.\")\n",
        "        if not path_ok: logging.error(f\"  - Video path issue. Current _confirmed_video_path: '{_confirmed_video_path}'.\")\n",
        "        if not depth_ok: logging.error(\"  - Depth model issue (_depth_model is None).\")\n",
        "        if not yolo_ok: logging.error(\"  - YOLO model issue (_yolo_model is None).\")\n",
        "        if not model_type_ok: logging.error(\"  - MODEL_TYPE not defined.\")\n",
        "\n",
        "print(\"✅ Cell 6: WebSocket Server (BEV with Blinking/Sized Dots & Trails).\")\n",
        "print(\"   Set LOGGING_LEVEL. ENABLE_VIDEO_WRITING to save.\")\n",
        "print(\"   Run 'run_websocket_server()' in Cell 7 to start.\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7t1oQtYyLspq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "26901b30-706e-49ac-fdb0-b1ed12d80591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initiating Server Start Sequence (Cell 7) ---\n",
            "✅ Prerequisite check: Depth model found.\n",
            "✅ Prerequisite check: YOLO model found.\n",
            "✅ Prerequisite check: Video file found at '/content/Generate_Crowded_Street_Scene_Video.mp4'.\n",
            "✅ Prerequisite check: Ngrok token seems set.\n",
            "\n",
            "All critical prerequisites met. Calling 'run_websocket_server()'...\n",
            "Server will now start. Execution will block here until stopped (e.g., by KeyboardInterrupt).\n",
            "Look for the 'wss://...' URL (if using Ngrok) or 'ws://localhost...' URL in the logs above to connect your client.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Attempting to clear CUDA cache before starting server...\n",
            "CUDA cache cleared.\n",
            "\n",
            "======================================================================\n",
            "=== WS Server Ready ===\n",
            "=== Connect client to: wss://aff947d81a8a.ngrok-free.app ===\n",
            "======================================================================\n",
            "\n",
            "Offering download: /content/processed_video_output.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_86ccd4d9-60f3-4459-900b-9cb560034088\", \"processed_video_output.mp4\", 22428740)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Offering download: /content/processed_video_output.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3e7408a2-db15-4a38-829e-589ff7530ca6\", \"processed_video_output.mp4\", 31975430)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Ngrok disconnect error: ngrok client exception, URLError: [Errno 111] Connection refused\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1765815143.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Call the main function defined in Cell 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mrun_websocket_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# --- Code below runs only AFTER server stops ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1919074250.py\u001b[0m in \u001b[0;36mrun_websocket_server\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdepth_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0myolo_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel_type_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All prerequisites met. Starting server event loop...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prerequisites not met. Server startup aborted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title 7. Start the Streaming Server\n",
        "import os\n",
        "import torch # For torch.cuda.is_available() and empty_cache()\n",
        "\n",
        "print(\"--- Initiating Server Start Sequence (Cell 7) ---\")\n",
        "\n",
        "# --- Prerequisite Checks (Final verification before calling run_websocket_server) ---\n",
        "# These check the global variables that should have been set by earlier cells.\n",
        "prerequisites_met = True\n",
        "\n",
        "# Check 1: Depth model (should be loaded in Cell 2 and globally accessible as _depth_model)\n",
        "if '_depth_model' not in globals() or globals()['_depth_model'] is None:\n",
        "    print(\"❌ ERROR (Cell 7): Depth model (_depth_model) not loaded or not in global scope.\")\n",
        "    prerequisites_met = False\n",
        "else:\n",
        "    print(\"✅ Prerequisite check: Depth model found.\")\n",
        "\n",
        "# Check 2: YOLO model (should be loaded in Cell 2 and globally accessible as _yolo_model)\n",
        "if '_yolo_model' not in globals() or globals()['_yolo_model'] is None:\n",
        "    print(\"❌ ERROR (Cell 7): YOLO model (_yolo_model) not loaded or not in global scope.\")\n",
        "    prerequisites_met = False\n",
        "else:\n",
        "    print(\"✅ Prerequisite check: YOLO model found.\")\n",
        "\n",
        "# Check 3: Video path (should be set by Cell 4 as input_video_path, then used by run_websocket_server)\n",
        "# run_websocket_server itself will check _confirmed_video_path\n",
        "# Here we check the input_video_path that run_websocket_server will use\n",
        "temp_confirmed_video_path = None\n",
        "if 'input_video_path' in globals() and globals()['input_video_path'] is not None:\n",
        "    temp_confirmed_video_path = globals()['input_video_path']\n",
        "\n",
        "if temp_confirmed_video_path is None:\n",
        "    print(\"❌ ERROR (Cell 7): Video path (input_video_path) not set. Run Cell 4.\")\n",
        "    prerequisites_met = False\n",
        "elif not os.path.exists(temp_confirmed_video_path):\n",
        "    print(f\"❌ ERROR (Cell 7): Video file missing: '{temp_confirmed_video_path}'. Run Cell 4.\")\n",
        "    prerequisites_met = False\n",
        "else:\n",
        "    print(f\"✅ Prerequisite check: Video file found at '{temp_confirmed_video_path}'.\")\n",
        "\n",
        "# Check 4: Ngrok Token (from Cell 1, used by start_server called by run_websocket_server)\n",
        "if 'NGROK_AUTH_TOKEN' not in globals() or \\\n",
        "   globals()['NGROK_AUTH_TOKEN'] == \"YOUR_NGROK_AUTH_TOKEN\" or \\\n",
        "   not globals()['NGROK_AUTH_TOKEN']:\n",
        "    print(\"⚠️ WARNING (Cell 7): Ngrok Auth Token (NGROK_AUTH_TOKEN) missing or placeholder. Ngrok tunneling might fail or be skipped.\")\n",
        "    # Not setting prerequisites_met to False, as local server can still run.\n",
        "else:\n",
        "    print(\"✅ Prerequisite check: Ngrok token seems set.\")\n",
        "\n",
        "# --- Execute Server Start ---\n",
        "if prerequisites_met:\n",
        "    print(\"\\nAll critical prerequisites met. Calling 'run_websocket_server()'...\")\n",
        "    print(\"Server will now start. Execution will block here until stopped (e.g., by KeyboardInterrupt).\")\n",
        "    print(\"Look for the 'wss://...' URL (if using Ngrok) or 'ws://localhost...' URL in the logs above to connect your client.\")\n",
        "    print(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "    # Clear CUDA cache if GPU is used, before starting the blocking server call\n",
        "    if DEVICE == 'cuda' and torch.cuda.is_available():\n",
        "        print(\"Attempting to clear CUDA cache before starting server...\")\n",
        "        try:\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"CUDA cache cleared.\")\n",
        "        except Exception as e_cache:\n",
        "            print(f\"Warning: Error clearing CUDA cache: {e_cache}\")\n",
        "\n",
        "    # Call the main function defined in Cell 6\n",
        "    run_websocket_server() # This is a blocking call\n",
        "\n",
        "    # --- Code below runs only AFTER server stops ---\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"--- Server execution finished or interrupted (Cell 7) ---\")\n",
        "else:\n",
        "    print(\"\\n--- ❌ Server Start Aborted due to missing prerequisites (Cell 7) ---\")\n",
        "    print(\"--- Please check the error messages above and ensure all setup cells (1-4) have run successfully. ---\")\n",
        "\n",
        "print(\"-\" * 70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}